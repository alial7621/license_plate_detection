{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1ET7bXF8WPjJ3uMoHd7nrpX9Yg5xyOOK0","authorship_tag":"ABX9TyP32wS4O7jMyS5kTaqDwd0j"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Train model"],"metadata":{"id":"Ymy9aNYzTr9o"}},{"cell_type":"code","source":["%cd /content/gdrive/MyDrive\n","!git clone https://github.com/WongKinYiu/yolov7.git\n","%cd yolov7\n","!pip install -r requirements.txt\n","!pip install roboflow"],"metadata":{"id":"BVyzLMSw3fON"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fa1ZnWzzVN9_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678208134431,"user_tz":-210,"elapsed":35844,"user":{"displayName":"ali alsh","userId":"16363363027563158752"}},"outputId":"b4c16616-ac90-4e7f-f138-e7168dd7df48"},"outputs":[{"output_type":"stream","name":"stdout","text":["loading Roboflow workspace...\n","loading Roboflow project...\n","Downloading Dataset Version Zip in License-plate-1 to yolov7pytorch: 97% [131555328 / 135569799] bytes"]},{"output_type":"stream","name":"stderr","text":["Extracting Dataset Version Zip to License-plate-1 in yolov7pytorch:: 100%|██████████| 8648/8648 [00:02<00:00, 3376.89it/s]\n"]}],"source":["from roboflow import Roboflow\n","rf = Roboflow(api_key=\"RyZweMUNaHCXeg9yceug\")\n","project = rf.workspace(\"ali-wntbu\").project(\"license-plate-ccqf1\")\n","dataset = project.version(1).download(\"yolov7\")"]},{"cell_type":"code","source":["%%bash\n","wget -P /content/yolov7 https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt"],"metadata":{"id":"pHwQRYJC6NZv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python train.py --batch 16 --cfg cfg/training/yolov7.yaml --epochs 40 --data /content/yolov7/License-plate-1/data.yaml --weights 'yolov7.pt'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VxHon1pL6sKe","executionInfo":{"status":"ok","timestamp":1678212780931,"user_tz":-210,"elapsed":4626619,"user":{"displayName":"ali alsh","userId":"16363363027563158752"}},"outputId":"3c0db2ad-df31-4a10-922f-2d388443e1ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["YOLOR 🚀 v0.1-122-g3b41c2c torch 1.13.1+cu116 CUDA:0 (Tesla T4, 15101.8125MB)\n","\n","Namespace(adam=False, artifact_alias='latest', batch_size=16, bbox_interval=-1, bucket='', cache_images=False, cfg='cfg/training/yolov7.yaml', data='/content/yolov7/License-plate-1/data.yaml', device='', entity=None, epochs=40, evolve=False, exist_ok=False, freeze=[0], global_rank=-1, hyp='data/hyp.scratch.p5.yaml', image_weights=False, img_size=[640, 640], label_smoothing=0.0, linear_lr=False, local_rank=-1, multi_scale=False, name='exp', noautoanchor=False, nosave=False, notest=False, project='runs/train', quad=False, rect=False, resume=False, save_dir='runs/train/exp', save_period=-1, single_cls=False, sync_bn=False, total_batch_size=16, upload_dataset=False, v5_metric=False, weights='yolov7.pt', workers=8, world_size=1)\n","\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n","2023-03-07 16:56:00.716345: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-03-07 16:56:01.598423: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n","2023-03-07 16:56:01.598551: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n","2023-03-07 16:56:01.598571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.2, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.0, paste_in=0.15, loss_ota=1\n","\u001b[34m\u001b[1mwandb: \u001b[0mInstall Weights & Biases for YOLOR logging with 'pip install wandb' (recommended)\n","Overriding model.yaml nc=80 with nc=1\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1       928  models.common.Conv                      [3, 32, 3, 1]                 \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n","  5                -2  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n","  6                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  7                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  8                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  9                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 10  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 11                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n"," 12                -1  1         0  models.common.MP                        []                            \n"," 13                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 14                -3  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 16          [-1, -3]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 18                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 19                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 20                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 22                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 23  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 24                -1  1    263168  models.common.Conv                      [512, 512, 1, 1]              \n"," 25                -1  1         0  models.common.MP                        []                            \n"," 26                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 27                -3  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 28                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 29          [-1, -3]  1         0  models.common.Concat                    [1]                           \n"," 30                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 31                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 32                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 33                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 34                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 35                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 36  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 37                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n"," 38                -1  1         0  models.common.MP                        []                            \n"," 39                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 40                -3  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 41                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n"," 42          [-1, -3]  1         0  models.common.Concat                    [1]                           \n"," 43                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 44                -2  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 45                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 46                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 47                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 48                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 49  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 50                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n"," 51                -1  1   7609344  models.common.SPPCSPC                   [1024, 512, 1]                \n"," 52                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 53                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 54                37  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 55          [-1, -2]  1         0  models.common.Concat                    [1]                           \n"," 56                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 57                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 58                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n"," 59                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 60                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 61                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 62[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 63                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 64                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 65                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 66                24  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n"," 67          [-1, -2]  1         0  models.common.Concat                    [1]                           \n"," 68                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 69                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 70                -1  1     73856  models.common.Conv                      [128, 64, 3, 1]               \n"," 71                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 72                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 73                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 74[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 75                -1  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n"," 76                -1  1         0  models.common.MP                        []                            \n"," 77                -1  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n"," 78                -3  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n"," 79                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 80      [-1, -3, 63]  1         0  models.common.Concat                    [1]                           \n"," 81                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 82                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 83                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n"," 84                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 85                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 86                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 87[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 88                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 89                -1  1         0  models.common.MP                        []                            \n"," 90                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n"," 91                -3  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n"," 92                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 93      [-1, -3, 51]  1         0  models.common.Concat                    [1]                           \n"," 94                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 95                -2  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 96                -1  1   1180160  models.common.Conv                      [512, 256, 3, 1]              \n"," 97                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 98                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 99                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n","100[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n","101                -1  1   1049600  models.common.Conv                      [2048, 512, 1, 1]             \n","102                75  1    328704  models.common.RepConv                   [128, 256, 3, 1]              \n","103                88  1   1312768  models.common.RepConv                   [256, 512, 3, 1]              \n","104               101  1   5246976  models.common.RepConv                   [512, 1024, 3, 1]             \n","105   [102, 103, 104]  1     34156  models.yolo.IDetect                     [1, [[12, 16, 19, 36, 40, 28], [36, 75, 76, 55, 72, 146], [142, 110, 192, 243, 459, 401]], [256, 512, 1024]]\n","/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","Model Summary: 415 layers, 37196556 parameters, 37196556 gradients, 105.1 GFLOPS\n","\n","Transferred 552/566 items from yolov7.pt\n","Scaled weight_decay = 0.0005\n","Optimizer groups: 95 .bias, 95 conv.weight, 98 other\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'License-plate-1/train/labels' images and labels... 3767 found, 0 missing, 0 empty, 0 corrupted: 100% 3767/3767 [00:01<00:00, 2357.36it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: License-plate-1/train/labels.cache\n","\u001b[34m\u001b[1mval: \u001b[0mScanning 'License-plate-1/valid/labels' images and labels... 372 found, 0 missing, 0 empty, 0 corrupted: 100% 372/372 [00:00<00:00, 2307.33it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: License-plate-1/valid/labels.cache\n","\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 4.45, Best Possible Recall (BPR) = 1.0000\n","Image sizes 640 train, 640 test\n","Using 2 dataloader workers\n","Logging results to runs/train/exp\n","Starting training for 40 epochs...\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","      0/39     7.83G   0.06095  0.009641         0   0.07059        13       640: 100% 236/236 [04:05<00:00,  1.04s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 12/12 [00:20<00:00,  1.72s/it]\n","                 all         372         381       0.723       0.774       0.752       0.337\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","      1/39     10.7G   0.04484  0.006719         0   0.05156        15       640: 100% 236/236 [03:29<00:00,  1.12it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 12/12 [00:06<00:00,  1.78it/s]\n","                 all         372         381       0.854       0.847       0.869       0.367\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","      2/39     10.7G   0.03731  0.005761         0   0.04307        14       640: 100% 236/236 [03:28<00:00,  1.13it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 12/12 [00:07<00:00,  1.61it/s]\n","                 all         372         381       0.913       0.908       0.946       0.533\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","      3/39     10.7G   0.03437  0.004534         0    0.0389        14       640: 100% 236/236 [03:26<00:00,  1.14it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 12/12 [00:06<00:00,  1.79it/s]\n","                 all         372         381       0.944       0.934       0.954       0.592\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","      4/39     10.7G   0.03438  0.004118         0    0.0385         9       640: 100% 236/236 [03:27<00:00,  1.14it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 12/12 [00:07<00:00,  1.66it/s]\n","                 all         372         381       0.896       0.858       0.892       0.487\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","      5/39     10.7G   0.03174  0.003915         0   0.03566         9       640: 100% 236/236 [03:27<00:00,  1.14it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 12/12 [00:07<00:00,  1.68it/s]\n","                 all         372         381       0.928       0.942       0.962       0.588\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","      6/39     10.7G   0.02983  0.003708         0   0.03354        18       640: 100% 236/236 [03:26<00:00,  1.15it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 12/12 [00:06<00:00,  1.73it/s]\n","                 all         372         381       0.962       0.926       0.966       0.589\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","      7/39     10.7G   0.02878  0.003358         0   0.03214        16       640: 100% 236/236 [03:28<00:00,  1.13it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 12/12 [00:07<00:00,  1.66it/s]\n","                 all         372         381       0.973       0.932       0.969         0.6\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","      8/39     10.7G   0.02748  0.003218         0    0.0307        17       640: 100% 236/236 [03:28<00:00,  1.13it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 12/12 [00:06<00:00,  1.79it/s]\n","                 all         372         381       0.976        0.95        0.97        0.66\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","      9/39     10.7G   0.02658   0.00293         0   0.02951         8       640: 100% 236/236 [03:26<00:00,  1.14it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 12/12 [00:07<00:00,  1.67it/s]\n","                 all         372         381       0.971       0.971       0.983       0.669\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","     10/39     10.7G   0.02541  0.002887         0    0.0283        22       640: 100% 236/236 [03:28<00:00,  1.13it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 12/12 [00:07<00:00,  1.71it/s]\n","                 all         372         381       0.941       0.971       0.977       0.663\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","     11/39     10.7G   0.02463  0.002882         0   0.02751        23       640: 100% 236/236 [03:28<00:00,  1.13it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 12/12 [00:06<00:00,  1.78it/s]\n","                 all         372         381       0.979       0.976       0.984       0.637\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","     12/39     10.7G   0.02409  0.002801         0    0.0269         9       640: 100% 236/236 [03:25<00:00,  1.15it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 12/12 [00:07<00:00,  1.68it/s]\n","                 all         372         381       0.973       0.958        0.98       0.672\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","     13/39     10.7G   0.02319  0.002812         0     0.026        20       640: 100% 236/236 [03:26<00:00,  1.14it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 12/12 [00:06<00:00,  1.80it/s]\n","                 all         372         381       0.981        0.95       0.981       0.656\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","     14/39     10.7G   0.02301  0.002823         0   0.02583        24       640: 100% 236/236 [03:28<00:00,  1.13it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 12/12 [00:07<00:00,  1.69it/s]\n","                 all         372         381       0.986       0.966       0.989       0.675\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","     15/39     10.7G   0.02196  0.002709         0   0.02467        16       640: 100% 236/236 [03:26<00:00,  1.15it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 12/12 [00:06<00:00,  1.81it/s]\n","                 all         372         381       0.981       0.971       0.986       0.664\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","     16/39     10.7G   0.02186  0.002694         0   0.02455        11       640: 100% 236/236 [03:27<00:00,  1.14it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 12/12 [00:07<00:00,  1.65it/s]\n","                 all         372         381       0.979       0.969       0.981       0.657\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","     17/39     10.7G   0.02153  0.002673         0    0.0242        11       640: 100% 236/236 [03:28<00:00,  1.13it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 12/12 [00:06<00:00,  1.79it/s]\n","                 all         372         381       0.984       0.966       0.984       0.692\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","     18/39     10.7G   0.02069   0.00267         0   0.02336        14       640: 100% 236/236 [03:26<00:00,  1.14it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 12/12 [00:07<00:00,  1.68it/s]\n","                 all         372         381       0.992       0.945       0.978        0.64\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","     19/39     10.7G   0.02059  0.002537         0   0.02313        11       640: 100% 236/236 [03:26<00:00,  1.14it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 12/12 [00:06<00:00,  1.80it/s]\n","                 all         372         381       0.987       0.984        0.99       0.706\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","     20/39     10.7G   0.02051  0.002576         0   0.02308        22       640: 100% 236/236 [03:28<00:00,  1.13it/s]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 12/12 [00:07<00:00,  1.62it/s]\n","                 all         372         381       0.973       0.963       0.979        0.69\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","     21/39     10.7G    0.0216  0.002728         0   0.02433        42       640:   4% 10/236 [00:09<03:43,  1.01it/s]\n","Traceback (most recent call last):\n","  File \"train.py\", line 616, in <module>\n","    train(hyp, opt, device, tb_writer)\n","  File \"train.py\", line 372, in train\n","    scaler.scale(loss).backward()\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\", line 488, in backward\n","    torch.autograd.backward(\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\", line 197, in backward\n","    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","KeyboardInterrupt\n","^C\n"]}]},{"cell_type":"markdown","source":["# Load the model and get predicted plate"],"metadata":{"id":"ko1QKMs_TxIQ"}},{"cell_type":"code","source":["import fileinput\n","import os\n","from pathlib import Path\n","from typing import Union\n","import torch\n","import cv2 as cv\n","import numpy as np\n","import re\n","import matplotlib.pyplot as plt"],"metadata":{"id":"Lrnrn9YtRn4L","executionInfo":{"status":"ok","timestamp":1678232585433,"user_tz":-210,"elapsed":3311,"user":{"displayName":"ali alsh","userId":"16363363027563158752"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["def prepend_text(filename: Union[str, Path], text: str):\n","    with fileinput.input(filename, inplace=True) as file:\n","        for line in file:\n","            if file.isfirstline():\n","                print(text)\n","            print(line, end=\"\")\n","            \n","if not os.path.isdir('yolov7'):\n","    yolov7_repo_url = 'https://github.com/WongKinYiu/yolov7'\n","    os.system(f'git clone {yolov7_repo_url}')\n","    # Fix import errors\n","    for file in ['yolov7/models/common.py', 'yolov7/models/experimental.py', 'yolov7/models/yolo.py', 'yolov7/utils/datasets.py']:\n","         prepend_text(file, \"import sys\\nsys.path.insert(0, './yolov7')\")"],"metadata":{"id":"ziQ9B8nPRw3h","executionInfo":{"status":"ok","timestamp":1678232588309,"user_tz":-210,"elapsed":2893,"user":{"displayName":"ali alsh","userId":"16363363027563158752"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["%cd /content/yolov7"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cNzlt2nZW5xe","executionInfo":{"status":"ok","timestamp":1678232588310,"user_tz":-210,"elapsed":8,"user":{"displayName":"ali alsh","userId":"16363363027563158752"}},"outputId":"0b9c22c9-29c5-4927-9eb7-446d3ca16544"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/yolov7\n"]}]},{"cell_type":"code","source":["from yolov7.models.experimental import attempt_load\n","from yolov7.utils.general import check_img_size\n","from yolov7.utils.torch_utils import select_device, TracedModel\n","from yolov7.utils.datasets import letterbox\n","from yolov7.utils.general import non_max_suppression, scale_coords\n","from yolov7.utils.plots import plot_one_box"],"metadata":{"id":"6tJEHeXgR2Vh","executionInfo":{"status":"ok","timestamp":1678232590576,"user_tz":-210,"elapsed":2270,"user":{"displayName":"ali alsh","userId":"16363363027563158752"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["weights = '/content/drive/MyDrive/best.pt'\n","device_id = 'cpu'\n","image_size = 640\n","trace = True\n","\n","# Initialize\n","device = select_device(device_id)\n","half = device.type != 'cpu'  # half precision only supported on CUDA\n","\n","# Load model\n","model = attempt_load(weights, map_location=device)  # load FP32 model\n","stride = int(model.stride.max())  # model stride\n","imgsz = check_img_size(image_size, s=stride)  # check img_size\n","\n","if trace:\n","    model = TracedModel(model, device, image_size)\n","\n","if half:\n","    model.half()  # to FP16\n","    \n","if device.type != 'cpu':\n","    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n","\n","model.eval()"],"metadata":{"id":"kP6PRo-tR7kY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def detect_plate(source_image):\n","    # Padded resize\n","    img_size = 640\n","    stride = 32\n","    img = letterbox(source_image, img_size, stride=stride)[0]\n","\n","    # Convert\n","    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n","    img = np.ascontiguousarray(img)\n","    img = torch.from_numpy(img).to(device)\n","    img = img.half() if half else img.float()  # uint8 to fp16/32\n","    img /= 255.0  # 0 - 255 to 0.0 - 1.0\n","    if img.ndimension() == 3:\n","        img = img.unsqueeze(0)\n","        \n","    with torch.no_grad():\n","        # Inference\n","        pred = model(img, augment=True)[0]\n","\n","    # Apply NMS\n","    pred = non_max_suppression(pred, 0.25, 0.45, classes=0, agnostic=True)\n","\n","    plate_detections = []\n","    det_confidences = []\n","    \n","    # Process detections\n","    for i, det in enumerate(pred):  # detections per image\n","        if len(det):\n","            # Rescale boxes from img_size to im0 size\n","            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], source_image.shape).round()\n","\n","            # Return results\n","            for *xyxy, conf, cls in reversed(det):\n","                coords = [int(position) for position in (torch.tensor(xyxy).view(1, 4)).tolist()[0]]\n","                plate_detections.append(coords)\n","                det_confidences.append(conf.item())\n","\n","    return plate_detections, det_confidences\n","\n","def crop(image, coord):\n","    cropped_image = image[int(coord[1]):int(coord[3]), int(coord[0]):int(coord[2])]\n","    return cropped_image\n","\n","def draw_box_img(source_image, plate_detections):\n","    for coords in plate_detections:\n","        cv.rectangle(source_image, (coords[0], coords[1]), (coords[2], coords[3]), (0,255,0), 2)\n","    cv.imwrite(\"/content/detected_image.jpg\", source_image)"],"metadata":{"id":"L4dhlJTuST0n","executionInfo":{"status":"ok","timestamp":1678234313590,"user_tz":-210,"elapsed":376,"user":{"displayName":"ali alsh","userId":"16363363027563158752"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["source_image_path = \"/content/drive/MyDrive/datasets/License-plate-1/test/images/191_-129-_png.rf.e5fc8f0ef650931ba230112a349ab44a.jpg\"    \n","source_image = cv.imread(source_image_path)\n","print(source_image.shape)\n","# Padded resize\n","img_size = 480\n","stride = 32\n","img = letterbox(source_image, img_size, stride=stride)[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bEK-S1r4SGfQ","executionInfo":{"status":"ok","timestamp":1678232634403,"user_tz":-210,"elapsed":2566,"user":{"displayName":"ali alsh","userId":"16363363027563158752"}},"outputId":"137a01f8-1fb5-4aef-c0c3-96fca5ebb695"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["(480, 480, 3)\n"]}]},{"cell_type":"code","source":["plate_detections, det_confidences = detect_plate(img)\n","print(f'confidence score: {det_confidences}')\n","draw_box_img(source_image, plate_detections)\n","plate = crop(source_image, plate_detections[0])\n","gray = cv.cvtColor(plate, cv.COLOR_BGR2GRAY)"],"metadata":{"id":"oyjj7l9uTDBs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cv.imwrite(\"/content/plate.jpg\", plate)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hJ3FaEMzZgPI","executionInfo":{"status":"ok","timestamp":1678234277782,"user_tz":-210,"elapsed":393,"user":{"displayName":"ali alsh","userId":"16363363027563158752"}},"outputId":"ae70401b-b1fc-4fc3-e27f-827aea08700b"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","source":["# OCR"],"metadata":{"id":"9hHbskHWUgc0"}},{"cell_type":"code","source":["# Apply Gaussian blurring and thresholding \n","# to reveal the characters on the license plate\n","blurred = cv.GaussianBlur(gray, (5, 5), 0)\n","thresh = cv.adaptiveThreshold(blurred, 255, cv.ADAPTIVE_THRESH_MEAN_C,\n","                               cv.THRESH_BINARY_INV, 45, 15)"],"metadata":{"id":"dgLVYmrnUkUl","executionInfo":{"status":"ok","timestamp":1678232672512,"user_tz":-210,"elapsed":376,"user":{"displayName":"ali alsh","userId":"16363363027563158752"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Perform connected components analysis on the thresholded image and\n","# initialize the mask to hold only the components we are interested in\n","_, labels = cv.connectedComponents(thresh)\n","mask = np.zeros(thresh.shape, dtype=\"uint8\")"],"metadata":{"id":"UKBjL6RFVFjn","executionInfo":{"status":"ok","timestamp":1678232675702,"user_tz":-210,"elapsed":5,"user":{"displayName":"ali alsh","userId":"16363363027563158752"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Set lower bound and upper bound criteria for characters\n","total_pixels = plate.shape[0] * plate.shape[1]\n","lower = total_pixels // 70 # heuristic param, can be fine tuned if necessary\n","upper = total_pixels // 20 # heuristic param, can be fine tuned if necessary"],"metadata":{"id":"eFuQnJDUVv76","executionInfo":{"status":"ok","timestamp":1678232677283,"user_tz":-210,"elapsed":3,"user":{"displayName":"ali alsh","userId":"16363363027563158752"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Loop over the unique components\n","for (i, label) in enumerate(np.unique(labels)):\n","    # If this is the background label, ignore it\n","    if label == 0:\n","        continue\n"," \n","    # Otherwise, construct the label mask to display only connected component\n","    # for the current label\n","    labelMask = np.zeros(thresh.shape, dtype=\"uint8\")\n","    labelMask[labels == label] = 255\n","    numPixels = cv.countNonZero(labelMask)\n"," \n","    # If the number of pixels in the component is between lower bound and upper bound, \n","    # add it to our mask\n","    if numPixels > lower and numPixels < upper:\n","        mask = cv.add(mask, labelMask)"],"metadata":{"id":"op5CTylvV80z","executionInfo":{"status":"ok","timestamp":1678232680346,"user_tz":-210,"elapsed":592,"user":{"displayName":"ali alsh","userId":"16363363027563158752"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# Find contours and get bounding box for each contour\n","cnts, _ = cv.findContours(mask.copy(), cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n","boundingBoxes = [cv.boundingRect(c) for c in cnts]"],"metadata":{"id":"8Mfy2oumWCKd","executionInfo":{"status":"ok","timestamp":1678232682323,"user_tz":-210,"elapsed":2,"user":{"displayName":"ali alsh","userId":"16363363027563158752"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["import functools\n","# Sort the bounding boxes from left to right, top to bottom\n","# sort by Y first, and then sort by X if Ys are similar\n","def compare(rect1, rect2):\n","    if abs(rect1[1] - rect2[1]) > 10:\n","        return rect1[1] - rect2[1]\n","    else:\n","        return rect1[0] - rect2[0]\n","boundingBoxes = sorted(boundingBoxes, key=functools.cmp_to_key(compare))"],"metadata":{"id":"KdITQ11sWJh1","executionInfo":{"status":"ok","timestamp":1678232683684,"user_tz":-210,"elapsed":3,"user":{"displayName":"ali alsh","userId":"16363363027563158752"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["classes = {\n","0:\t'0',\n","1:\t'1',\n","2:\t'2',\n","3:\t'3',\n","4:\t'4',\n","5:\t'5',\n","6:\t'6',\n","7:\t'7',\n","8:\t'8',\n","9:\t'9',\n","10:\t'الف',\n","11:\t'ب',\t\n","12:\t'پ',\t\n","13:\t'ج',\t\n","14:\t'ه',\t\n","15:\t'د',\t\n","16:\t'س',\t\n","17:\t'ص',\t\n","18:\t'ط',\t\n","19: 'ق',\n","20:\t'ل',\t\n","21:\t'م',\t\n","22:\t'ن',\t\n","23:\t'و',\t\n","24:\t'ی',\n","25:\t'ع',\t\n","26:\t'ت',\n","27: 'PwD'\n","}"],"metadata":{"id":"qVP0xBhUiYao","executionInfo":{"status":"ok","timestamp":1678234030815,"user_tz":-210,"elapsed":369,"user":{"displayName":"ali alsh","userId":"16363363027563158752"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["from tensorflow import keras\n","keras_model = keras.models.load_model('/content/char_pred.h5')\n","\n","plate_num = []\n","for bbox in boundingBoxes:\n","    # get char from the plate\n","    x, y, w, h = bbox\n","    bbox = [x, y, x+w, y+h]\n","    char_img = crop(plate, bbox)\n","    # resize to 64*64\n","    char_img = cv.resize(char_img, (64, 64))\n","    # predict keras model\n","    char_img = np.expand_dims(char_img, axis=0)\n","    pred = keras_model([char_img])\n","    idx = np.argsort(pred)[0]\n","    plate_num.append(classes[idx[-1]])\n","\n","for num in plate_num:\n","  print(num)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wb-9ywo2WMP9","executionInfo":{"status":"ok","timestamp":1678234165177,"user_tz":-210,"elapsed":353,"user":{"displayName":"ali alsh","userId":"16363363027563158752"}},"outputId":"605e96de-af5c-4445-ffa9-104e5c869abd"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["5\n","6\n","پ\n","9\n","5\n","3\n","7\n","ق\n"]}]},{"cell_type":"markdown","source":["# Train Keras Model"],"metadata":{"id":"-46QRr8lkOsc"}},{"cell_type":"markdown","source":["train keras model for char detection"],"metadata":{"id":"PmZ2UHGrkUqp"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HLM8B7kXA2kc","executionInfo":{"status":"ok","timestamp":1678228012145,"user_tz":-210,"elapsed":3708,"user":{"displayName":"Ali Lashini","userId":"10984931612286267126"}},"outputId":"8e371162-9577-48a0-8bcf-6b4ce50f7f4a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 67085 images belonging to 28 classes.\n","Found 16759 images belonging to 28 classes.\n"]}],"source":["from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","train_datagen = ImageDataGenerator(horizontal_flip=True,\n","                                   rotation_range=5,  \n","                                   width_shift_range=0.05,\n","                                   height_shift_range=0.05,\n","                                   validation_split=0.2)\n","\n","train_generator = train_datagen.flow_from_directory(\n","                  '/content/train',\n","                  target_size=(64, 64),\n","                  batch_size=32,\n","                  class_mode='categorical', \n","                  subset='training')\n","\n","val_generator = train_datagen.flow_from_directory(\n","                  '/content/train',\n","                  target_size=(64, 64),\n","                  batch_size=32,\n","                  class_mode='categorical', \n","                  subset='validation')"]},{"cell_type":"code","source":["# Define Your Model HERE!\n","from tensorflow.keras import Sequential\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","model = keras.Sequential()\n","model.add(keras.layers.Conv2D(filters=128, kernel_size=(3, 3), input_shape=(64, 64, 3), activation='relu'))\n","model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n","model.add(keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n","model.add(keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n","model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n","model.add(keras.layers.Flatten())\n","model.add(layers.Dense(64, activation='relu'))\n","model.add(keras.layers.Dense(units=len(classes), activation='softmax'))\n","\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1LYPibFlKT9R","executionInfo":{"status":"ok","timestamp":1678228014930,"user_tz":-210,"elapsed":2790,"user":{"displayName":"Ali Lashini","userId":"10984931612286267126"}},"outputId":"9de05056-69e7-4e65-b21b-4389f8caebd9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 62, 62, 128)       3584      \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 31, 31, 128)      0         \n"," )                                                               \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 29, 29, 32)        36896     \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 27, 27, 64)        18496     \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 13, 13, 64)       0         \n"," 2D)                                                             \n","                                                                 \n"," flatten (Flatten)           (None, 10816)             0         \n","                                                                 \n"," dense (Dense)               (None, 64)                692288    \n","                                                                 \n"," dense_1 (Dense)             (None, 28)                1820      \n","                                                                 \n","=================================================================\n","Total params: 753,084\n","Trainable params: 753,084\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n","\n","model.fit(train_generator,\n","          epochs=10,\n","          validation_data=val_generator)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qsGMptHEElCC","executionInfo":{"status":"ok","timestamp":1678229644677,"user_tz":-210,"elapsed":1629753,"user":{"displayName":"Ali Lashini","userId":"10984931612286267126"}},"outputId":"d535815f-9b5b-4b4d-dcc8-18f4c3c1174c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","2097/2097 [==============================] - 171s 79ms/step - loss: 0.7654 - acc: 0.8031 - val_loss: 0.3690 - val_acc: 0.8949\n","Epoch 2/10\n","2097/2097 [==============================] - 158s 75ms/step - loss: 0.1797 - acc: 0.9508 - val_loss: 0.2987 - val_acc: 0.9251\n","Epoch 3/10\n","2097/2097 [==============================] - 157s 75ms/step - loss: 0.1270 - acc: 0.9660 - val_loss: 0.2139 - val_acc: 0.9468\n","Epoch 4/10\n","2097/2097 [==============================] - 155s 74ms/step - loss: 0.0979 - acc: 0.9742 - val_loss: 0.2231 - val_acc: 0.9505\n","Epoch 5/10\n","2097/2097 [==============================] - 157s 75ms/step - loss: 0.0860 - acc: 0.9772 - val_loss: 0.2820 - val_acc: 0.9488\n","Epoch 6/10\n","2097/2097 [==============================] - 154s 74ms/step - loss: 0.0780 - acc: 0.9789 - val_loss: 0.2659 - val_acc: 0.9423\n","Epoch 7/10\n","2097/2097 [==============================] - 168s 80ms/step - loss: 0.0597 - acc: 0.9846 - val_loss: 0.2687 - val_acc: 0.9539\n","Epoch 8/10\n","2097/2097 [==============================] - 155s 74ms/step - loss: 0.0583 - acc: 0.9842 - val_loss: 0.1912 - val_acc: 0.9526\n","Epoch 9/10\n","2097/2097 [==============================] - 153s 73ms/step - loss: 0.0551 - acc: 0.9863 - val_loss: 0.1667 - val_acc: 0.9591\n","Epoch 10/10\n","2097/2097 [==============================] - 154s 73ms/step - loss: 0.0437 - acc: 0.9890 - val_loss: 0.1974 - val_acc: 0.9620\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7ff039cd6040>"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["model.save('/content/char_pred.h5')"],"metadata":{"id":"fUbkpH3nOSm6"},"execution_count":null,"outputs":[]}]}